{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _peek_data_shard(filename):\n",
    "    # only reads the header, returns header data\n",
    "    with open(filename, \"rb\") as f:\n",
    "        # first read the header, which is 256 int32 integers (4 bytes each)\n",
    "        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n",
    "    if header[0] != 20240520:\n",
    "        print(\"ERROR: magic number mismatch in the data .bin file!\")\n",
    "        print(\"---> HINT: Are you passing in a correct file with --input_bin?\")\n",
    "        print(\"---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README\")\n",
    "        print(\"---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try\")\n",
    "        exit(1)\n",
    "    assert header[1] == 1, \"unsupported version\"\n",
    "    ntok = header[2] # number of tokens (claimed)\n",
    "    return ntok # for now just return the number of tokens\n",
    "\n",
    "def _load_data_shard(filename):\n",
    "    with open(filename, \"rb\") as f:\n",
    "        # first read the header, which is 256 int32 integers (4 bytes each)\n",
    "        header = np.frombuffer(f.read(256*4), dtype=np.int32)\n",
    "        assert header[0] == 20240520, \"magic number mismatch in the data .bin file\"\n",
    "        assert header[1] == 1, \"unsupported version\"\n",
    "        ntok = header[2] # number of tokens (claimed)\n",
    "        # the rest of it are tokens, stored as uint16\n",
    "        tokens = np.frombuffer(f.read(), dtype=np.uint16)\n",
    "    assert len(tokens) == ntok, \"number of tokens read does not match header?\"\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_peek_data_shard('data/fineweb10B/fineweb_train_000074.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self, filename_pattern, B, T, process_rank, num_processes):\n",
    "        self.process_rank = process_rank\n",
    "        self.num_processes = num_processes\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "\n",
    "        # glob files that match the pattern\n",
    "        self.files = sorted(glob.glob(filename_pattern))\n",
    "        assert len(self.files) > 0, f\"did not find any files that match the pattern {filename_pattern}\"\n",
    "\n",
    "        # load and validate all data shards, count number of tokens in total\n",
    "        ntok_total = 0\n",
    "        for fname in self.files:\n",
    "            shard_ntok = _peek_data_shard(fname)\n",
    "            assert shard_ntok >= num_processes * B * T + 1\n",
    "            ntok_total += shard_ntok\n",
    "        self.ntok_total = ntok_total\n",
    "        print(f\"DataLoader: total number of tokens: {ntok_total:,} across {len(self.files)} files\")\n",
    "\n",
    "        # kick things off\n",
    "        self.current_shard = None\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        # we're being a bit clever here: if we already had shard 0 loaded,\n",
    "        # then don't do the work to reload it, just reset the pointer\n",
    "        if self.current_shard != 0:\n",
    "            self.current_shard = 0\n",
    "            self.tokens = _load_data_shard(self.files[self.current_shard])\n",
    "        self.current_position = self.process_rank * self.B * self.T\n",
    "\n",
    "    def advance(self): # advance to next data shard\n",
    "        self.current_shard = (self.current_shard + 1) % len(self.files)\n",
    "        self.current_position = self.process_rank * self.B * self.T\n",
    "        self.tokens = _load_data_shard(self.files[self.current_shard])\n",
    "\n",
    "    def next_batch(self):\n",
    "        B = self.B\n",
    "        T = self.T\n",
    "        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n",
    "        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)\n",
    "        x = (buf[:-1]).view(B, T) # inputs\n",
    "        y = (buf[1:]).view(B, T) # targets\n",
    "        # advance the start pointer in current shard\n",
    "        self.current_position += B * T * self.num_processes\n",
    "        # if loading the next batch would be out of bounds advance the shard\n",
    "        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n",
    "            self.advance()\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader: total number of tokens: 10,255,324,043 across 103 files\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(\"data/fineweb10B/fineweb_train_*.bin\", 4, 64, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = train_loader.next_batch()\n",
    "x, y = x.to(\"cuda\"), y.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 64])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 64])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader: total number of tokens: 100,000,000 across 1 files\n"
     ]
    }
   ],
   "source": [
    "val_loader = DataLoader(\"data/fineweb10B/fineweb_val_*.bin\", 4, 64, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.n_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0018"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1.8e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch23",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
